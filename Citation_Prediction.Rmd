---
title: "Regression"
author: "Zhe GUAN"
date: "2024-12-28"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(tidyr)
library(dplyr)
library(tidytext)
library(wordcloud2)
library(textstem)
library(ggplot2)
library(ggpubr)
library(topicmodels)
library(tidyverse)
library(stm)
```

```{r}

#read data
journal_data <- read.csv("/Users/zheguan/DDA/Datamining/assignment4/journal_data.csv")
journal_data <- tibble(journal_data)
journal_data <- journal_data %>%
  dplyr::mutate(row_id = dplyr::row_number())

#prepare a column with title-abstract and do lemmatization
journal_data$title_abstract <- paste0(journal_data$title," - ",journal_data$abstract)
journal_data <- journal_data %>% mutate(title_abstract = lemmatize_strings(title_abstract))

```

```{r}

journal_token <- journal_data %>%
  select(row_id, title_abstract, year, journal) %>%
  unnest_tokens(output = word, input = title_abstract, token = "ngrams", n = 2) %>% 
  separate(word, into = c("word1", "word2"), sep = " ") %>%  
  filter(!word1 %in% stop_words$word, !word2 %in% stop_words$word) %>% 
  filter(!grepl("\\b\\d+\\b", word1), !grepl("\\b\\d+\\b", word2)) %>% 
  unite("bigram", word1, word2, sep = "-")


dtm <- journal_token %>%
  count(row_id, bigram) %>%
  cast_dtm(document = row_id, term = bigram, value = n)

meta_data <- journal_data %>%
  distinct(row_id, year, journal) 

documents <- journal_token %>%
  group_by(row_id) %>%
  summarise(text = paste(bigram, collapse = " "))

processed <- textProcessor(
  documents = documents$text,
  metadata = meta_data,
  customstopwords = NULL
)


prep <- prepDocuments(processed$documents, processed$vocab, processed$meta)
docs <- prep$documents
vocab <- prep$vocab
meta <- prep$meta


```


```{r}
search_results <- searchK(
  documents = docs,
  vocab = vocab,
  K = seq(2, 12, by = 2),  
  prevalence = ~ year+journal,
  data = meta,
  max.em.its = 75,
  seed = 1234
)

search_results
```
```{r}

results <- data.frame(
  K = as.numeric(search_results$results$K),
  exclus = as.numeric(search_results$results$exclus),
  semcoh = as.numeric(search_results$results$semcoh),
  heldout = as.numeric(search_results$results$heldout),
  residual = as.numeric(search_results$results$residual),
  bound = as.numeric(search_results$results$bound)
)


ggplot(results, aes(x = K)) +
  geom_line(aes(y = exclus, color = "Exclusivity")) +
  geom_line(aes(y = semcoh, color = "Semantic Coherence")) +
  scale_color_manual(name = "Metrics", values = c("Exclusivity" = "blue", "Semantic Coherence" = "red")) +
  labs(
    title = "Exclusivity vs. Semantic Coherence",
    x = "Number of Topics (K)", y = "Value"
  ) +
  theme_minimal()


```

```{r}

dtm_model <- stm(
  documents = docs,
  vocab = vocab,
  K = 6,  
  prevalence = ~ year + journal,  
  data = meta,
  max.em.its = 75,
  seed = 1234
)
```
```{r}
labelTopics(dtm_model)
```

```{r}
journal_data
```
```{r}
topic_summary <- dtm_model$theta |> as.data.frame() 
topic_summary
```
```{r}
#check which documents are removed when we train the topic model.
prep <- prepDocuments(processed$documents, processed$vocab, processed$meta)

removed_docs <- setdiff(seq_len(nrow(meta_data)), prep$meta$row_id)
print(removed_docs)
```

```{r}
all_docs <- seq_len(nrow(meta_data))


complete_theta <- matrix(0, nrow = length(all_docs), ncol = ncol(dtm_model$theta))

complete_theta[prep$meta$row_id, ] <- dtm_model$theta
topic_summary <- as.data.frame(complete_theta)
topic_summary
```

```{r}
time_effect <- estimateEffect(
  formula = c(1:6) ~ year,  
  stmobj = dtm_model,
  metadata = meta,
  uncertainty = "Global"
)

effect_data <- data.frame()


for (topic in 1:6) {
  plot_data <- plot(
    time_effect,
    covariate = "year",
    method = "continuous",
    topics = topic,
    print = FALSE
  )
  

  topic_data <- data.frame(
    year = plot_data$x,
    topic = paste0("Topic ", topic),
    estimate = plot_data$means[[1]],
    lower = plot_data$ci[[1]]["2.5%", ],
    upper = plot_data$ci[[1]]["97.5%", ]
  )
  effect_data <- rbind(effect_data, topic_data)
}


print(effect_data)


p <- ggplot(effect_data, aes(x = year, y = estimate, color = topic)) +
  geom_line(size = 1.2) +
  geom_ribbon(aes(ymin = lower, ymax = upper, fill = topic), alpha = 0.2) +
  facet_wrap(~ topic, scales = "free_y") +  
  labs(
    title = "Topic Trends Over Time",
    x = "Year",
    y = "Estimated Topic Proportion"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5, size = 20, face = "bold"),
    axis.title = element_text(size = 16),
    axis.text = element_text(size = 10),
    legend.position = "none" 
  )

print(p)
```

```{r}
journal_data <- journal_data %>% cbind(topic_summary)
journal_data
```

```{r}
source("/Users/zheguan/DDA/Datamining/assignment4/R/functions.R")
journal_token <- journal_data %>%
    select(row_id, title_abstract) %>%
    unnest_tokens(output = word, title_abstract, token = "words")
  
tfidf_table <- tf_idf_table(journal_token)
tfidf_table
```
```{r}
tfidf_wide <- tfidf_table %>%
  select(index_paper, word, tf_idf) %>%  
  pivot_wider(names_from = word,   
              values_from = tf_idf,  
              values_fill = 0)       


tfidf_wide
```

```{r}
sample_fraction <- 0.8

train_row_ids <- journal_data %>%
  group_by(journal) %>%
  sample_frac(sample_fraction) %>%
  pull(row_id) 

train_row_ids
```
```{r}
train_sample_tfidf <- tfidf_wide %>% filter(index_paper %in% train_row_ids)
train_sample_tfidf
```
```{r}
tfidf_matrix <- train_sample_tfidf %>%
  select(-index_paper) %>% 
  as.matrix()

constant_columns <- apply(tfidf_matrix, 2, function(x) var(x) == 0)
zero_columns <- colSums(tfidf_matrix) == 0
columns_to_remove <- constant_columns | zero_columns
sum(columns_to_remove) 
tfidf_matrix_clean <- tfidf_matrix[, !columns_to_remove]


pca_model <- prcomp(tfidf_matrix, center = TRUE)


```

```{r}

explained_variance <- (pca_model$sdev)^2 / sum((pca_model$sdev)^2)


cumulative_variance <- cumsum(explained_variance)


explained_df <- data.frame(
  Principal_Component = seq_along(explained_variance),
  Explained_Variance = explained_variance,
  Cumulative_Variance = cumulative_variance
)

```

```{r}
library(ggplot2)


ggplot(explained_df, aes(x = Principal_Component)) +
  geom_bar(aes(y = Explained_Variance), stat = "identity", fill = "skyblue") +
  geom_line(aes(y = Cumulative_Variance, group = 1), color = "red", size = 1) +
  geom_point(aes(y = Cumulative_Variance), color = "red", size = 2) +
  labs(
    title = "Explained Variance by Principal Components",
    x = "Principal Component",
    y = "Variance Explained"
  ) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12)
  )+geom_hline(yintercept = 0.9, linetype = "dashed", color = "blue")

```

```{r}
journal_data <- journal_data %>%
  mutate(author_count = str_count(authors, "\\s+") + 1)

head(journal_data)
```



```{r}
train_data_re
```
```{r}
library(caret)

dummy_model <- dummyVars(~ journal, data = journal_data)


one_hot_journal <- predict(dummy_model, newdata = journal_data)


dim(one_hot_journal)


journal_levels <- colnames(one_hot_journal)  

journal_data_onehot <- cbind(journal_data, one_hot_journal)

journal_data_onehot
```
```{r}
journal_data_re <- journal_data_onehot %>% select(year, pages, citations, row_id, V1, V2, V3, V4, V5, V6, author_count, `journalHealth Systems`, `journalJournal of Simulation`, `journalJournal of the Operational Research Society`)
colnames(journal_data_re) <- colnames(journal_data_re) %>%
  gsub("^V(\\d+)$", "topic_\\1_prob", .) %>%
  gsub("^journal", "", .) %>% 
  gsub(" ", "_", .)
journal_data_re
```

```{r}
train_data_re <- journal_data_re %>% filter(row_id %in% train_row_ids) %>% select(-row_id)
test_data_re <- journal_data_re %>% filter(!(row_id %in% train_row_ids)) %>% select(-row_id)
```

```{r}

lm_model <- lm(citations ~ ., data = train_data_re)

summary(lm_model)

```


```{r}
library(caret)
library(xgboost)
library(doParallel)

cl <- makeCluster(detectCores() - 1)  
registerDoParallel(cl)


grid <- expand.grid(
  nrounds = c(100, 200),          
  eta = c(0.01, 0.1, 0.3),             
  max_depth = c(3, 6),             
  gamma = c(0, 1, 3),                  
  colsample_bytree = c(0.6, 0.8, 1.0), 
  min_child_weight = c(1, 5, 10),      
  subsample = c(0.6, 0.8, 1.0)
)


train_control <- trainControl(
  method = "cv",       
  number = 3,          
  verboseIter = FALSE,  
  allowParallel = TRUE, 
  search = "random"
)


xgb_grid <- train(
  x = as.matrix(train_data_re[, -which(names(train_data_re) == "citations")]),
  y = train_data_re$citations,
  method = "xgbTree",
  trControl = train_control,
  tuneGrid = grid,
  metric = "RMSE",
  tuneLength = 500,
  watchlist = watchlist,
  verbose = 1 
)


print(xgb_grid$bestTune)

saveRDS(xgb_grid, "xgb_grid_tuning.rds")

stopCluster(cl)  
registerDoSEQ()  

```

```{r}
best_xgb_model <- xgb_grid$finalModel
```


```{r}
dtrain <- xgb.DMatrix(data = as.matrix(train_data_re[, -which(names(train_data_re) == "citations")]), label = train_data_re$citations)
dtest <- xgb.DMatrix(data = as.matrix(test_data_re[, -which(names(test_data_re) == "citations")]), label = test_data_re$citations)

watchlist <- list(train = dtrain, test = dtest)

best_params <- xgb_grid$bestTune

xgb_params <- list(
  objective = "reg:squarederror",
  eta = best_params$eta,
  max_depth = best_params$max_depth,
  gamma = best_params$gamma,
  colsample_bytree = best_params$colsample_bytree,
  min_child_weight = best_params$min_child_weight,
  subsample = best_params$subsample
)

xgb_model <- xgb.train(
  params = xgb_params,
  data = dtrain,
  nrounds = best_params$nrounds,
  watchlist = watchlist,
  eval_metric = "rmse",
  verbose = 1
)

train_log <- xgb_model$evaluation_log
print(train_log)


```

```{r}
library(ggplot2)

ggplot(train_log, aes(x = iter)) +
  geom_line(aes(y = train_rmse, color = "Train RMSE"), size = 1) +
  geom_line(aes(y = test_rmse, color = "Test RMSE"), size = 1) +
  labs(
    title = "Learning Curve",
    x = "Iteration",
    y = "RMSE"
  ) +
  scale_color_manual(
    name = "Error Type",
    values = c("Train RMSE" = "blue", "Test RMSE" = "red")
  ) +
  theme_minimal()

```

```{r}
library(SHAPforxgboost)

train_matrix <- as.matrix(train_data_re[, -which(names(train_data_re) == "citations")])

shap_values <- shap.values(
  xgb_model = xgb_grid$finalModel,
  X_train = train_matrix
)

shap_importance <- shap_values$mean_shap_score

shap_long <- shap.prep(
  shap_contrib = shap_values$shap_score,
  X_train = train_matrix
)

shap.plot.summary(shap_long)

```

```{r}
library(doParallel)

cl <- makeCluster(detectCores() - 1)  
registerDoParallel(cl)

grid <- expand.grid(
  mtry = c(2, 3, 5, 7)      
)

train_control <- trainControl(
  method = "cv",
  number = 3,
  verboseIter = TRUE,
  allowParallel = TRUE
)


rf_grid <- train(
  citations ~ .,
  data = train_data_re,
  method = "rf",
  trControl = train_control,
  tuneGrid = grid,
  metric = "RMSE",
  ntree = 500
)


print(rf_grid$bestTune)
stopCluster(cl)  
registerDoSEQ() 
```

```{r}
library(doParallel)
library(caret)
library(randomForest)
library(ggplot2)


cl <- makeCluster(detectCores() - 1)  
registerDoParallel(cl)


grid <- expand.grid(
  mtry = c(2, 3, 5, 7)  
)


train_control <- trainControl(
  method = "cv",        
  number = 3,           
  verboseIter = TRUE,   
  allowParallel = TRUE  
)


rf_grid <- train(
  citations ~ .,            
  data = train_data_re,     
  method = "rf",            
  trControl = train_control,
  tuneGrid = grid,
  metric = "RMSE",
  ntree = 500               
)


print(rf_grid$bestTune)


rf_model <- randomForest(
  citations ~ .,
  data = train_data_re,
  mtry = rf_grid$bestTune$mtry,
  ntree = 500,
  importance = TRUE,
  keep.inbag = TRUE
)


oob_error <- rf_model$mse
iterations <- 1:rf_model$ntree


learning_curve <- data.frame(
  Iterations = iterations,
  OOB_Error = sqrt(oob_error)  
)


ggplot(learning_curve, aes(x = Iterations, y = OOB_Error)) +
  geom_line(color = "blue", size = 1) +
  labs(
    title = "Learning Curve of Random Forest",
    x = "Number of Trees",
    y = "OOB RMSE"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12)
  )


stopCluster(cl)
registerDoSEQ()

```

```{r}
library(kernelshap)
library(shapviz)
library(doParallel)

cl <- makeCluster(detectCores() - 1)  # 使用多核
registerDoParallel(cl)

features <- train_data_re[, -which(names(train_data_re) == "citations")]
target <- train_data_re$citations

# 选择背景样本 (random subsample of features)
bg_X <- features[sample(1:nrow(features), 100), ]

# 计算 SHAP 值
shap_values <- kernelshap(
  object = xgb_grid$finalModel,      # 训练好的模型
  X = features[1:4000, ] %>% as.matrix(),  # 需要解释的前 100 个样本
  bg_X = bg_X %>% as.matrix()
)


# 使用 shapviz 进行可视化
sv <- shapviz(shap_values)
# 绘制 Beeswarm 图
sv_importance(sv, kind = "bee")  



# 绘制单特征依赖图
sv_dependence(sv, v = "pages")  # 根据需要替换特征名

# 绘制条形图
sv_importance(sv, kind = "bar")
stopCluster(cl)  # 停止并释放多核集群
registerDoSEQ()  # 恢复到默认的单线程执行
```
```{r}

# 计算 SHAP 值
shap_values_rf <- kernelshap(
  object = rf_model,      # 训练好的模型
  X = features[1:2000, ] ,  # 需要解释的前 100 个样本
  bg_X = bg_X 
)


# 使用 shapviz 进行可视化
sv_rf <- shapviz(shap_values_rf)
# 绘制 Beeswarm 图
sv_importance(sv_rf, kind = "bee")  



# 绘制单特征依赖图
sv_dependence(sv_rf, v = "pages")  # 根据需要替换特征名

# 绘制条形图
sv_importance(sv_rf, kind = "bar")

```
```{r}
library(Metrics)
library(dplyr)



lm_predictions <- predict(lm_model, test_data_re)
rf_predictions <- predict(rf_model, test_data_re)
xgb_predictions <- predict(xgb_model, dtest)

results <- data.frame(
  Model = c("Linear Regression", "Random Forest", "XGBoost"),
  MSE = c(
    mse(test_data_re$citations, lm_predictions),
    mse(test_data_re$citations, rf_predictions),
    mse(test_label, xgb_predictions)
  ),
  RMSE = c(
    rmse(test_data_re$citations, lm_predictions),
    rmse(test_data_re$citations, rf_predictions),
    rmse(test_label, xgb_predictions)
  ),
  R2 = c(
    cor(test_data_re$citations, lm_predictions)^2,
    cor(test_data_re$citations, rf_predictions)^2,
    cor(test_label, xgb_predictions)^2
  )
)

print(results)
```

```{r}
library(ggplot2)
library(tidyr)

results_long <- results %>%
  pivot_longer(cols = c(MSE, RMSE, R2), names_to = "Metric", values_to = "Value")

ggplot(results_long, aes(x = Model, y = Value, group = Metric, color = Metric)) +
  geom_point(size = 3) +
  geom_line(size = 1) +
  facet_wrap(~Metric, scales = "free", ncol = 1) + 
  labs(
    title = "Comparison of Regression Models Across Metrics",
    x = "Model",
    y = "Value"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12),
    legend.position = "none"  
  )
```

```{r}

lm_residuals <- test_data_re$citations - lm_predictions
rf_residuals <- test_data_re$citations - rf_predictions
xgb_residuals <- test_label - xgb_predictions


residual_data <- data.frame(
  Residuals = c(lm_residuals, rf_residuals, xgb_residuals),
  Model = rep(c("Linear Regression", "Random Forest", "XGBoost"), each = length(lm_residuals))
)

ggplot(residual_data, aes(x = Residuals, fill = Model)) +
  geom_histogram(binwidth = 5, alpha = 0.7, position = "identity") +
  labs(
    title = "Residual Distribution Across Models",
    x = "Residuals",
    y = "Frequency"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10)
  )

```
```{r}

ggplot(residual_data, aes(x = Residuals, color = Model, fill = Model)) +
  geom_density(alpha = 0.4) +  
  labs(
    title = "Residual Density Across Models",
    x = "Residuals",
    y = "Density"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10)
  )+
  xlim(-10,50)

```
```{r}

lm_residuals <- test_data_re$citations - lm_predictions
rf_residuals <- test_data_re$citations - rf_predictions
xgb_residuals <- test_label - xgb_predictions


lm_relative_residuals <- lm_residuals / test_data_re$citations
rf_relative_residuals <- rf_residuals / test_data_re$citations
xgb_relative_residuals <- xgb_residuals / test_label


relative_residual_data <- data.frame(
  RelativeResiduals = c(lm_relative_residuals, rf_relative_residuals, xgb_relative_residuals),
  Model = rep(c("Linear Regression", "Random Forest", "XGBoost"), each = length(lm_relative_residuals))
)


ggplot(relative_residual_data, aes(x = RelativeResiduals, fill = Model)) +
  geom_density(alpha = 0.7) +
  labs(
    title = "Relative Residual Distribution Across Models",
    x = "Relative Residuals",
    y = "Density"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10)
  )+
  xlim(-30,10)

```

```{r}
library(ggplot2)


residual_data <- data.frame(
  Residuals = c(lm_residuals, rf_residuals, xgb_residuals),
  Model = rep(c("Linear Regression", "Random Forest", "XGBoost"), each = length(lm_residuals))
)


ggplot(residual_data, aes(x = Model, y = Residuals, fill = Model)) +
  geom_boxplot(outlier.color = "red", outlier.shape = 16, alpha = 0.7) +
  labs(
    title = "Residual Boxplot Across Models",
    x = "Model",
    y = "Residuals"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12),
    legend.position = "none"
  )

```
```{r}

residual_data_cdf <- residual_data %>%
  group_by(Model) %>%
  arrange(Residuals) %>%
  mutate(CDF = ecdf(Residuals)(Residuals))
residual_data_cdf

ggplot(residual_data_cdf, aes(x = Residuals, y = CDF, color = Model)) +
  geom_line(size = 1) +
  labs(
    title = "CDF of Residuals Across Models",
    x = "Residuals",
    y = "Cumulative Probability"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10)
  )+
  xlim(-30, 30)

```
```{r}
ks.test(rf_residuals, xgb_residuals)

```


```{r}
input_abstract <- "Productivity growth of institutions of higher education is of interest for two main reasons: education is an important factor for productivity growth of the economy, and in countries where higher education is funded by the public sector, accountability of resource use is of key interest. Educational services consist of teaching, research and the “third mission” of dissemination of knowledge to the society at large. A bootstrapped Malmquist productivity change index is used to calculate productivity development for Norwegian institutions of higher education over the 10-year period 2004–2013. The confidence intervals from bootstrapping allow part of the uncertainty of point estimates stemming from sample variation to be revealed. The main result is that the majority of institutions have had a positive productivity growth over the total period. However, when comparing with growth in labour input, the impact on productivity varies a lot."
input_year <-"2017"
input_journal_name <- "Journal of Simulation"


```

```{r}
input_title_abstract <- paste0(input_title, " - ", input_abstract)
input_title_abstract <- lemmatize_strings(input_title_abstract)
input_title_abstract
input_token <- tibble(
  row_id = 1,  # 假设这是第一个文档
  title_abstract = input_title_abstract,
  year = input_year,
  journal = input_journal_name
)  %>%
  unnest_tokens(output = word, input = title_abstract, token = "ngrams", n = 2) %>% 
  separate(word, into = c("word1", "word2"), sep = " ") %>%  
  filter(!word1 %in% stop_words$word, !word2 %in% stop_words$word) %>% 
  filter(!grepl("\\b\\d+\\b", word1), !grepl("\\b\\d+\\b", word2)) %>% 
  unite("bigram", word1, word2, sep = "-")
print(input_token)
```
```{r}
library(data.table)
input_token
input_token_dt <- as.data.table(input_token)
input_token_dt <- input_token_dt[, .(text = paste(bigram, collapse = " ")), by = row_id]
input_token_dt |> as.data.frame()
```

```{r}

processed_new_doc <- textProcessor(
  documents = input_token_dt$text,
  metadata = data.frame(year = input_year, journal = input_journal_name),
  customstopwords = NULL
)

prep_new <- prepDocuments(processed_new_doc$documents, processed_new_doc$vocab, processed_new_doc$meta, lower.thresh = 1)
docs_new <- prep$documents
vocab_new <- prep$vocab
meta_new <- prep$meta


aligned_new_doc <- alignCorpus(
  old.vocab = dtm_model$vocab,
  new = processed_new_doc
)



```


```{r}
new_doc_theta <- fitNewDocuments(
  model = dtm_model,
  documents = aligned_new_doc$documents
)$theta

new_doc_theta
```

```{r}
test_data_re
```
```{r}
residual_data_cdf <- residual_data %>%
  group_by(Model) %>%
  arrange(Residuals) %>%
  mutate(CDF = ecdf(Residuals)(Residuals))


ggplot(residual_data_cdf, aes(x = Residuals, y = CDF, color = Model)) +
  geom_line(size = 1) +
  labs(
    title = "CDF of Residuals Across Models",
    x = "Residuals",
    y = "Cumulative Probability"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10)
  )+
  xlim(-10, 10)
```


```{r}

# 用户输入的期刊名称
input_journal_name <- "Journal of Simulation"
input_pages <- "16"
input_author_count <-"3"
input_abstract <- "Productivity growth of institutions of higher education is of interest for two main reasons: education is an important factor for productivity growth of the economy, and in countries where higher education is funded by the public sector, accountability of resource use is of key interest. Educational services consist of teaching, research and the “third mission” of dissemination of knowledge to the society at large. A bootstrapped Malmquist productivity change index is used to calculate productivity development for Norwegian institutions of higher education over the 10-year period 2004–2013. The confidence intervals from bootstrapping allow part of the uncertainty of point estimates stemming from sample variation to be revealed. The main result is that the majority of institutions have had a positive productivity growth over the total period. However, when comparing with growth in labour input, the impact on productivity varies a lot."
input_year <-"2017"
input_journal_name <- "Journal of Simulation"


data <- data.frame(
  year = as.integer(input_year),
  pages = as.integer(input_pages),
  topic_1_prob = new_doc_theta[1],
  topic_2_prob = new_doc_theta[2],
  topic_3_prob = new_doc_theta[3],
  topic_4_prob = new_doc_theta[4],
  topic_5_prob = new_doc_theta[5],
  topic_6_prob = new_doc_theta[6],
  author_count = as.integer(input_author_count),
  row_id = 1, 
  Health_Systems = 0,
  Journal_of_Simulation = 0,
  Journal_of_the_Operational_Research_Society = 0
)


journal_columns <- c("Health_Systems", "Journal_of_Simulation", "Journal_of_the_Operational_Research_Society")
data <- data %>%
  mutate(across(all_of(journal_columns), ~ ifelse(gsub("_", " ", cur_column()) == input_journal_name, 1, 0)))

```

```{r}
# 保存模型到文件
saveRDS(lm_model, "best_lm_model.rds")
saveRDS(best_rf_model, "best_rf_model.rds")
saveRDS(best_xgb_model, "best_xgb_model.rds")
saveRDS(dtm_model,"dtm_model_K6.rds")
```

```{r}
journal_data[3,"abstract"] |> as.character()
```

